---
layout: post
title: "Shannon, l'entropie et le chaos de nos boîtes mail"
---

# Shannon, l'entropie et le chaos de nos boîtes mail

Notifications, e-mails, articles, flux de réseaux sociaux... toute cette information nous rend-elle vraiment plus "savants" ? Pas si sûr. Pour y voir plus clair, un détour par les travaux de Claude Shannon et un focus sur le concept d'entropie est éclairant.

## L'Information comme réducteur d'incertitude

Dans les années 1940, le mathématicien Claude Shannon a posé les bases de la théorie de l'information. Son idée ? L'information n'est pas une "chose", mais une mesure de la **réduction de l'incertitude**.

Pour Shannon, la quantité d'information dans un message est proportionnelle à l'incertitude qu'il lève. Imaginez un jeu de "devine le nombre" entre 1 et 100. Si je vous dis "le nombre est pair", je vous donne une information précieuse qui réduit de moitié le champ des possibles. L'information, ici, a diminué votre "entropie".

## L'Entropie : plus qu'un concept de physique

Le terme "entropie" nous vient de la thermodynamique, où il désigne le degré de désordre d'un système. Shannon a transposé ce concept au monde de l'information. Pour lui, l'entropie est une mesure de l'incertitude ou du hasard.

* **Entropie élevée = Grande incertitude.** Le résultat d'un lancer de dé équilibré a une entropie élevée.
* **Entropie faible = Faible incertitude.** Le résultat d'une pièce truquée qui tombe toujours sur "face" a une entropie nulle. Le message "c'est face" ne nous apprend rien.

Recevoir de l'information, c'est donc faire baisser l'entropie de notre état de connaissance sur un sujet.

## De l'Information à la Connaissance : Le chaînon manquant

Mais suffit-il d'accumuler de l'information pour acquérir de la connaissance ? Non. Si les informations sont les briques, la connaissance est la maison que l'on construit avec.

* **Information :** Il pleut.
* **Connaissance :** En observant les nuages et la pression atmosphérique, je peux prédire qu'il va pleuvoir. C'est un modèle mental robuste qui me permet de comprendre les liens de cause à effet et d'anticiper.

La connaissance organise l'information pour créer une réduction durable et globale de notre "entropie" interne.

## La connaissance, notre force "néguentropique"

Le physicien Léon Brillouin a introduit le concept de **"néguentropie"** (entropie négative) pour décrire les forces qui luttent contre le désordre. La vie elle-même est un processus néguentropique.

À notre échelle, la connaissance est notre agent néguentropique. En apprenant et en reliant les concepts, nous ne faisons pas que collecter des "faits" : nous mettons de l'ordre dans notre compréhension du monde. Nous transformons une avalanche d'informations chaotiques (haute entropie) en un savoir structuré et actionnable (basse entropie).

> À l'ère de l'infobésité, il est plus que jamais nécessaire de ne pas confondre le signal et le bruit, l'information et la connaissance.

La prochaine fois que vous vous sentirez submergé par le flot d'informations, demandez-vous : "Cette information réduit-elle simplement mon incertitude immédiate, ou contribue-t-elle à construire une structure de connaissance plus solide ?"

La réponse à cette question peut transformer le chaos informationnel en clarté intellectuelle.
